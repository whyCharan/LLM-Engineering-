{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "414d57aa",
   "metadata": {},
   "source": [
    "# Getting Started With LangChain and OpenAi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9262678d",
   "metadata": {},
   "source": [
    "In this quickstart we'll see how  to: \n",
    "\n",
    "* Get setup with LangChain, Langsmith and LangServe\n",
    "\n",
    "* Use the most basic and common components of LangChain: Prompt templates, models and ouput parsers.\n",
    "\n",
    "* Build a simple application with LangChain\n",
    "\n",
    "* Trace your application with LangChain\n",
    "\n",
    "* Serve your application with LangServe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3ed0d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.8-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\chara\\onedrive\\desktop\\krish_naik\\langchain\\genv\\lib\\site-packages (from ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\chara\\onedrive\\desktop\\krish_naik\\langchain\\genv\\lib\\site-packages (from ipywidgets) (8.37.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\chara\\onedrive\\desktop\\krish_naik\\langchain\\genv\\lib\\site-packages (from ipywidgets) (5.14.3)\n",
      "Collecting widgetsnbextension~=4.0.14 (from ipywidgets)\n",
      "  Downloading widgetsnbextension-4.0.15-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab_widgets~=3.0.15 (from ipywidgets)\n",
      "  Downloading jupyterlab_widgets-3.0.16-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\chara\\onedrive\\desktop\\krish_naik\\langchain\\genv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: decorator in c:\\users\\chara\\onedrive\\desktop\\krish_naik\\langchain\\genv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\chara\\onedrive\\desktop\\krish_naik\\langchain\\genv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (1.3.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\chara\\onedrive\\desktop\\krish_naik\\langchain\\genv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\chara\\onedrive\\desktop\\krish_naik\\langchain\\genv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\chara\\onedrive\\desktop\\krish_naik\\langchain\\genv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\chara\\onedrive\\desktop\\krish_naik\\langchain\\genv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
      "Requirement already satisfied: stack_data in c:\\users\\chara\\onedrive\\desktop\\krish_naik\\langchain\\genv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: typing_extensions>=4.6 in c:\\users\\chara\\onedrive\\desktop\\krish_naik\\langchain\\genv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (4.15.0)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\chara\\onedrive\\desktop\\krish_naik\\langchain\\genv\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.14)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\chara\\onedrive\\desktop\\krish_naik\\langchain\\genv\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.5)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\chara\\onedrive\\desktop\\krish_naik\\langchain\\genv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\chara\\onedrive\\desktop\\krish_naik\\langchain\\genv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\chara\\onedrive\\desktop\\krish_naik\\langchain\\genv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Downloading ipywidgets-8.1.8-py3-none-any.whl (139 kB)\n",
      "Downloading jupyterlab_widgets-3.0.16-py3-none-any.whl (914 kB)\n",
      "   ---------------------------------------- 0.0/914.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 914.9/914.9 kB 10.5 MB/s  0:00:00\n",
      "Downloading widgetsnbextension-4.0.15-py3-none-any.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.2/2.2 MB 17.6 MB/s  0:00:00\n",
      "Installing collected packages: widgetsnbextension, jupyterlab_widgets, ipywidgets\n",
      "\n",
      "   ------------- -------------------------- 1/3 [jupyterlab_widgets]\n",
      "   -------------------------- ------------- 2/3 [ipywidgets]\n",
      "   -------------------------- ------------- 2/3 [ipywidgets]\n",
      "   -------------------------- ------------- 2/3 [ipywidgets]\n",
      "   -------------------------- ------------- 2/3 [ipywidgets]\n",
      "   -------------------------- ------------- 2/3 [ipywidgets]\n",
      "   -------------------------- ------------- 2/3 [ipywidgets]\n",
      "   -------------------------- ------------- 2/3 [ipywidgets]\n",
      "   -------------------------- ------------- 2/3 [ipywidgets]\n",
      "   -------------------------- ------------- 2/3 [ipywidgets]\n",
      "   -------------------------- ------------- 2/3 [ipywidgets]\n",
      "   -------------------------- ------------- 2/3 [ipywidgets]\n",
      "   -------------------------- ------------- 2/3 [ipywidgets]\n",
      "   -------------------------- ------------- 2/3 [ipywidgets]\n",
      "   -------------------------- ------------- 2/3 [ipywidgets]\n",
      "   -------------------------- ------------- 2/3 [ipywidgets]\n",
      "   -------------------------- ------------- 2/3 [ipywidgets]\n",
      "   -------------------------- ------------- 2/3 [ipywidgets]\n",
      "   ---------------------------------------- 3/3 [ipywidgets]\n",
      "\n",
      "Successfully installed ipywidgets-8.1.8 jupyterlab_widgets-3.0.16 widgetsnbextension-4.0.15\n"
     ]
    }
   ],
   "source": [
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85dda3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACKING_V2\"] = \"true\"\n",
    "os.environ['LANGCHAIN_PROJECT'] = os.getenv(\"LANGCHAIN_PROJECT\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "750b77ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model='gemma:2b' temperature=0.7\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model='gemma:2b', # or any model install via pull\n",
    "    temperature=0.7 \n",
    "\n",
    ")\n",
    "\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc9b29f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Thank you for asking! I'm doing well, thank you for asking. I'm happy to be here and ready to assist you with whatever you may need. \\n\\nHow can I help you today?\" additional_kwargs={} response_metadata={'model': 'gemma:2b', 'created_at': '2025-12-10T04:44:37.2635402Z', 'done': True, 'done_reason': 'stop', 'total_duration': 15467072100, 'load_duration': 8487466200, 'prompt_eval_count': 29, 'prompt_eval_duration': 1060313800, 'eval_count': 44, 'eval_duration': 5828733900, 'logprobs': None, 'model_name': 'gemma:2b', 'model_provider': 'ollama'} id='lc_run--019b0693-17a1-7a12-aef6-3a8f70a097bf-0' usage_metadata={'input_tokens': 29, 'output_tokens': 44, 'total_tokens': 73}\n"
     ]
    }
   ],
   "source": [
    "## Input and get response from LLM\n",
    "\n",
    "response = llm.invoke(\"Hello! How are you? \")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890576f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me answers based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### chat prompt Template\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",'You are an expert AI Engineer. Provide me answers based on the question'),\n",
    "        (\"user\",\"{input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4bc4e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Sure, here's a summary of Langsmith:\\n\\n**Langsmith** is an open-source language model focused on **factual and commonsense reasoning**. It excels at understanding and generating coherent and informative text, especially on topics related to **biology, medicine, and technology**.\\n\\n**Key features of Langsmith:**\\n\\n* **Fact-checking:** It relies on a massive dataset of verified facts and claims to identify inconsistencies and generate accurate summaries.\\n* **Generative abilities:** It can generate different creative text formats, including code, scripts, poems, and more.\\n* **Open-ended learning:** Its knowledge base is constantly growing as it encounters new and diverse data.\\n* **Domain-specific:** It has been fine-tuned on a massive dataset of scientific papers and textbooks, making it particularly strong in these domains.\\n\\n**Here are some additional things to know about Langsmith:**\\n\\n* It is a product of **Faiss**, a research lab focused on advancing language models through efficient and scalable machine learning.\\n* It is currently in **alpha version** and is still under active development.\\n* The company behind Langsmith is committed to open-sourcing its technology and promoting its use for research and education.\\n\\n**Here are some potential applications of Langsmith:**\\n\\n* **Language model training:** It can be used to improve the quality of existing language models by identifying and correcting factual errors.\\n* **Content creation:** It can be used to generate new content, such as scientific papers, code, or scripts.\\n* **Knowledge acquisition:** It can be used to learn new facts and concepts by interacting with it on a cognitive level.\\n\\nI hope this provides a helpful overview of Langsmith. Please let me know if you have any other questions.\" additional_kwargs={} response_metadata={'model': 'gemma:2b', 'created_at': '2025-12-10T04:51:15.1524717Z', 'done': True, 'done_reason': 'stop', 'total_duration': 74965843900, 'load_duration': 5031023800, 'prompt_eval_count': 44, 'prompt_eval_duration': 568356300, 'eval_count': 361, 'eval_duration': 68773766800, 'logprobs': None, 'model_name': 'gemma:2b', 'model_provider': 'ollama'} id='lc_run--019b0698-4178-7033-ae45-eebd799b04a7-0' usage_metadata={'input_tokens': 44, 'output_tokens': 361, 'total_tokens': 405}\n"
     ]
    }
   ],
   "source": [
    "## chain means we can combine things like prompts and llms\n",
    "\n",
    "chian = prompt|llm\n",
    "\n",
    "response = chian.invoke({\"input\":\"Can you tell me about Langsmith?\"})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66de841d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.messages.ai.AIMessage"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc9626bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Sure, I can help you with that.\\n\\n**Langsmith** is an open-source AI platform that focuses on building **safe and reliable large language models**. It is built on top of the **Jina** and **Megatron** frameworks, which are known for their robustness and safety.\\n\\n**Key features of Langsmith include:**\\n\\n* **Safe and reliable:** Langsmith models are rigorously tested and monitored to ensure they are free from biases, errors, and vulnerabilities.\\n* **Large size and power:** Langsmith models are significantly larger and more powerful than traditional language models, with trillions of parameters.\\n* **Security and privacy:** Langsmith takes security and privacy very seriously, with robust data protection and compliance measures in place.\\n* **Ethical AI:** The platform is designed to promote ethical AI, with features such as bias detection and impact assessment.\\n* **Open-source and collaborative:** Langsmith is an open-source project, allowing for community participation and collaboration.\\n\\n**Here are some additional details about Langsmith:**\\n\\n* It is developed by a team of AI researchers and engineers at Google Research.\\n* It is currently available in three versions: Langsmith 1.0, Langsmith 2.0, and Langsmith 3.0.\\n* It has been used in various applications, including language translation, text summarization, question answering, and code generation.\\n\\n**In conclusion, Langsmith is a robust, safe, and reliable language model that is suitable for a wide range of AI applications.**' additional_kwargs={} response_metadata={'model': 'gemma:2b', 'created_at': '2025-12-10T04:53:29.2063225Z', 'done': True, 'done_reason': 'stop', 'total_duration': 54092609400, 'load_duration': 318090900, 'prompt_eval_count': 44, 'prompt_eval_duration': 135968600, 'eval_count': 316, 'eval_duration': 53166746700, 'logprobs': None, 'model_name': 'gemma:2b', 'model_provider': 'ollama'} id='lc_run--019b069a-9ea8-7690-93b5-995cc5966026-0' usage_metadata={'input_tokens': 44, 'output_tokens': 316, 'total_tokens': 360}\n"
     ]
    }
   ],
   "source": [
    "## str output parser\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt|llm\n",
    "\n",
    "response = chain.invoke({'input':'Can you tell me about Langsmith?'})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0555c35d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
